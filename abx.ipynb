{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import ollama\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_SEQ_LEN = 4540 # Choose any! We auto support RoPE Scaling internally!\n",
    "DTYPE = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "LOAD_IN_4BIT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model we will generate two histories based on the same data, one will come from the finetuned model and the other from the base model. Then, by asking deepseek r1 14b which story it thinks is better, we will verify the success of the project. Deepseek r1 is based on super long CoT so we can see the motivation behind the choices made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fidning model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/patryk/Documents/tests/saved_model_summary',\n",
       " '/home/patryk/Documents/tests/saved_model_keywords_counter_top5',\n",
       " '/home/patryk/Documents/tests/saved_model_keywords_td_idf_only_nouns_top3',\n",
       " '/home/patryk/Documents/tests/saved_model_keywords_td_idf_top5',\n",
       " '/home/patryk/Documents/tests/saved_model_keywords_td_idf_top3',\n",
       " '/home/patryk/Documents/tests/saved_model_keywords_counter_top3',\n",
       " '/home/patryk/Documents/tests/saved_model_keywords_td_idf_only_nouns_top5']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_parths = []\n",
    "for item in os.listdir(os.getcwd()):\n",
    "    item_path = os.path.join(os.getcwd(), item)\n",
    "    if os.path.isdir(item_path) and item.startswith(\"saved_model\"):\n",
    "        models_parths.append(item_path)\n",
    "models_parths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output_keywords_td_idf_top3',\n",
       " 'output_keywords_counter_top5',\n",
       " 'training_notebook.ipynb',\n",
       " 'output_keywords_td_idf_only_nouns_top5',\n",
       " 'saved_model_summary',\n",
       " 'training_notebook copy.ipynb',\n",
       " 'output_keywords_td_idf_top5',\n",
       " 'abx.ipynb',\n",
       " 'output_keywords_counter_top3',\n",
       " 'output_summary',\n",
       " 'output_keywords_td_idf_only_nouns_top3',\n",
       " 'saved_model_keywords_counter_top5',\n",
       " 'saved_model_keywords_td_idf_only_nouns_top3',\n",
       " 'wandb',\n",
       " 'saved_model_keywords_td_idf_top5',\n",
       " 'saved_model_keywords_td_idf_top3',\n",
       " 'saved_model_keywords_counter_top3',\n",
       " 'saved_model_keywords_td_idf_only_nouns_top5',\n",
       " 'requirements',\n",
       " 'stories_summaries_keywords.csv',\n",
       " '_unsloth_temporary_saved_buffers']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/home/patryk/Documents/tests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"stories_summaries_keywords.csv\")\n",
    "data_df.rename(columns={\"text\": \"story\"}, inplace=True)\n",
    "data_df['story'] = data_df['story'].str.replace('\\n', ' ')\n",
    "data_df['story_length'] = data_df['story'].apply(len)\n",
    "data_df['story_length'].describe() # decyzja: packing=True vs uzywanie data_collator\n",
    "\n",
    "INPUT_COLUMNS = ['summary', 'keywords_counter_top5', 'keywords_counter_top3', 'keywords_td_idf_top5', 'keywords_td_idf_top3', 'keywords_td_idf_only_nouns_top5', 'keywords_td_idf_only_nouns_top3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(data_df)\n",
    "test = dataset.train_test_split(test_size=0.2, seed=42)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'story': 'One day, a boy named Peter went to the park. He ran around, playing with a ball and having fun. Suddenly, he saw a big sausage on the ground!  He went to pick it up, but instead he saw a friendly dog. It was a small, brown puppy called Lucy. She barked happily as she saw Peter and wagged her tail.  Peter was very surprised. He remembered his mum saying that he was not allowed to play with dogs. But Lucy seemed very nice, so Peter wanted to stay and play.  He held out his hand and Lucy came over. She felt very soft. Peter smiled and said hello. He even scratched Lucy behind the ears. Lucy was so pleased that she rolled over so Peter could wave her tummy.  Lucy was so smart. She even did a few tricks for Peter. He laughed and gave her the sausage he found. She gobbled it up and wagged her tail harder.   Peter and Lucy were soon best friends. They would go to the park every day to play and wave together.',\n",
       " 'summary': 'A young boy named Peter befriends a friendly, intelligent, and affectionate brown puppy named Lucy at the park, and despite initial reservations, they form a strong bond and become inseparable best friends.',\n",
       " 'keywords_counter_top5': \"['peter', 'lucy', 'saw', 'play', 'day']\",\n",
       " 'keywords_counter_top3': \"['peter', 'lucy', 'saw']\",\n",
       " 'td_idf': 'day boy name go park run around play ball fun saw big sausage ground go pick instead saw friendly dog small brown puppy call bark happily saw wag tail surprised remember mum say allow play dog seem nice want stay play hold hand come felt soft smile say hello even scratch behind ear pleased roll could wave tummy smart even trick laugh give sausage find gobble wag tail harder soon best friend would go park every day play wave together',\n",
       " 'td_idf_only_nouns': 'day boy park ball fun sausage ground dog puppy tail mum dog hand hello ear tummy trick sausage tail harder friend park day',\n",
       " 'keywords_td_idf_top5': \"['sausage', 'wag', 'tail', 'wave', 'gobble']\",\n",
       " 'keywords_td_idf_top3': \"['sausage', 'wag', 'tail']\",\n",
       " 'keywords_td_idf_only_nouns_top5': \"['sausage', 'tail', 'dog', 'harder', 'tummy']\",\n",
       " 'keywords_td_idf_only_nouns_top3': \"['sausage', 'tail', 'dog']\",\n",
       " 'story_length': 914}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABX code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_path, # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = MAX_SEQ_LEN + 200,\n",
    "        dtype = DTYPE,\n",
    "        load_in_4bit = LOAD_IN_4BIT,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inferenceFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.49.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060. Max memory: 11.66 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "tinyllama_model, tinyllama_tokenizer = load_model_and_tokenizer(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens = MAX_SEQ_LEN):\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        prompt\n",
    "    ], return_tensors = \"pt\").to(device)\n",
    "\n",
    "    output_ids = model.generate(**inputs, max_new_tokens = max_new_tokens)\n",
    "    decoded = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    return decoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_story(outut):\n",
    "    return outut.split(\"Story:\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column(model_path):\n",
    "    for column in INPUT_COLUMNS:\n",
    "        if model_path.endswith(column):\n",
    "            return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_NUM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.49.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060. Max memory: 11.66 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Model /home/patryk/Documents/tests/saved_model_summary loaded.\n",
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.49.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060. Max memory: 11.66 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Model /home/patryk/Documents/tests/saved_model_keywords_counter_top5 loaded.\n",
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.49.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060. Max memory: 11.66 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Model /home/patryk/Documents/tests/saved_model_keywords_td_idf_only_nouns_top3 loaded.\n",
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.49.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060. Max memory: 11.66 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Model /home/patryk/Documents/tests/saved_model_keywords_td_idf_top5 loaded.\n",
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.49.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060. Max memory: 11.66 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Model /home/patryk/Documents/tests/saved_model_keywords_td_idf_top3 loaded.\n",
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.49.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060. Max memory: 11.66 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Model /home/patryk/Documents/tests/saved_model_keywords_counter_top3 loaded.\n",
      "==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.49.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060. Max memory: 11.66 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Model /home/patryk/Documents/tests/saved_model_keywords_td_idf_only_nouns_top5 loaded.\n"
     ]
    }
   ],
   "source": [
    "scores = {model: [0, 0] for model in INPUT_COLUMNS} # [saved_model, tinyllama]\n",
    "for model_path in models_parths:\n",
    "\n",
    "    saved_model, saved_tokenizer = load_model_and_tokenizer(model_path)\n",
    "    print(f\"Model {model_path} loaded.\")\n",
    "\n",
    "    for i in range(TEST_NUM):\n",
    "        col_name = get_column(model_path)\n",
    "        INSTRUCTION = \"Summary:\\n\" if \"summary\" in model_path else \"Keywords:\\n\"\n",
    "\n",
    "        saved_model_story_promt = f\"{INSTRUCTION}{', '.join(eval(test[i][col_name])) if col_name != 'summary' else test[i][col_name]}Story:\\n\"\n",
    "        tinyllama_prompt = f\"Write a short story based on {'this summary:\\n' if 'summary' in model_path else 'these keywords:\\n'}{', '.join(eval(test[i][col_name])) if col_name != 'summary' else test[i][col_name]}\\nStory:\\n\"\n",
    "\n",
    "        saved_model_story = get_story(generate_text(saved_model, saved_tokenizer, saved_model_story_promt))\n",
    "        tinyllama_story = get_story(generate_text(tinyllama_model, tinyllama_tokenizer, tinyllama_prompt, max_new_tokens=600)) # zapetla sie - dlatego\n",
    "\n",
    "        # moze dodatkowo odwracac numeracje? - nie no based model jest nieporownywalnie slabszy nei ma co\n",
    "        deepseekr1_prompt = f\"You will get two stories 1 and 2. Your task is to choose the story that you think is better. Write the number of your chosen story as your answer. Your asnwer must be 1 or 2 and nothing more. Write just a single number.\\n\\n1. {saved_model_story}\\n\\n2. {tinyllama_story}\\n\\nYour answer:\\n\"\n",
    "        res = ollama.generate(keep_alive=5, model='deepseek-r1:14b', prompt=deepseekr1_prompt)['response']\n",
    "        if \"1\" in res.split(\"</think>\")[1].strip(): # 1 is better\n",
    "            scores[col_name][0] += 1\n",
    "        elif \"2\" in res.split(\"</think>\")[1].strip():\n",
    "            scores[col_name][1] += 1\n",
    "\n",
    "    saved_model = None\n",
    "    saved_tokenizer = None\n",
    "    gc.collect()   \n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': [10, 0],\n",
       " 'keywords_counter_top5': [9, 0],\n",
       " 'keywords_counter_top3': [9, 0],\n",
       " 'keywords_td_idf_top5': [10, 0],\n",
       " 'keywords_td_idf_top3': [9, 0],\n",
       " 'keywords_td_idf_only_nouns_top5': [10, 0],\n",
       " 'keywords_td_idf_only_nouns_top3': [10, 0]}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that in every case deepseek r1 chooses finetuned model over the base line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of deepseek r1 reasoning for the last pair of sotries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I\\'m trying to figure out which story is better between the two given. Let me read them carefully and analyze each one step by step.\\n\\nStarting with Story 1: It\\'s about a little girl named Lily who loves playing with her dolls and has a favorite red car. One day, her mom gives her a new finger with a shiny star, and Lily plays with it all day. Then her friend comes over and wants to play with the finger too, which Lily allows. However, when her mom sees this, she scolds Lily, saying the finger isn\\'t for playing and that it\\'s meant for her dolls. This makes Lily sad, and she argues with her mom about wanting to keep playing with the finger instead of her dolls. In the end, the mom explains that while Lily loves the finger, it\\'s not appropriate for playtime, so Lily learns an important lesson about respecting others\\' belongings and being kind.\\n\\nNow looking at Story 2: The title mentions a girl who is a friend of a boy, but when I read through it, all the sentences repeat \"The girl is a friend of a boy.\" It seems like the story was intended to have content explaining their friendship, but instead, it\\'s just a loop of the same sentence repeated multiple times. There’s no real plot development or narrative progression; it doesn\\'t tell any meaningful story beyond that repetitive line.\\n\\nComparing both stories: Story 1 has a clear narrative arc with characters (Lily and her friend), a conflict when Lily wants to play with the finger, and resolution where she learns a lesson. It teaches about respecting ownership and being kind, which are positive messages for children. The structure is coherent, making it engaging and suitable for its intended audience.\\n\\nIn contrast, Story 2 appears incomplete or perhaps was generated incorrectly. It lacks any plot, characters\\' development, conflict, or resolution. Repeating the same line over and over doesn\\'t create a story; instead, it\\'s just redundant text without meaning. This makes Story 2 ineffective as a narrative piece.\\n\\nTherefore, based on the content, structure, and the message conveyed, I would choose Story 1 as the better one because it presents a complete and meaningful tale with lessons for the reader.\\n</think>\\n\\n1'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
